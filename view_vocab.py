"""
æŸ¥çœ‹ nanochat tokenizer çš„è¯æ±‡è¡¨
"""
import pickle
import os

# åŠ è½½ tokenizer
tokenizer_path = os.path.expanduser("~/.cache/nanochat/tokenizer/tokenizer.pkl")
print(f"Loading tokenizer from: {tokenizer_path}\n")

with open(tokenizer_path, "rb") as f:
    enc = pickle.load(f)

# è·å–è¯æ±‡è¡¨ä¿¡æ¯
vocab_size = enc.n_vocab
print(f"è¯æ±‡è¡¨å¤§å°: {vocab_size:,} tokens\n")
print("=" * 80)

# æŸ¥çœ‹ç‰¹æ®Š tokens
print("\nğŸ”– ç‰¹æ®Š Tokens:")
print("-" * 80)
special_tokens = enc._special_tokens
for token_str, token_id in sorted(special_tokens.items(), key=lambda x: x[1]):
    print(f"ID {token_id:6d}: {token_str}")

# æŸ¥çœ‹å‰ 100 ä¸ªåŸºç¡€ tokens (0-99: å•å­—èŠ‚)
print("\nğŸ“ å‰ 100 ä¸ª Tokens (å•å­—èŠ‚ 0-99):")
print("-" * 80)
for i in range(0, 100, 10):
    tokens = []
    for j in range(i, min(i+10, 100)):
        try:
            decoded = enc.decode([j])
            # è½¬ä¹‰ä¸å¯è§å­—ç¬¦
            if decoded.isprintable():
                repr_str = decoded
            else:
                repr_str = repr(decoded)[1:-1]  # å»æ‰å¼•å·
            tokens.append(f"{j:3d}:{repr_str:5s}")
        except:
            tokens.append(f"{j:3d}:???")
    print(" | ".join(tokens))

# æŸ¥çœ‹ä¸€äº›å¸¸è§çš„ merged tokens
print("\nğŸ”¤ ç¤ºä¾‹ Merged Tokens (256-356):")
print("-" * 80)
for i in range(256, 357, 10):
    tokens = []
    for j in range(i, min(i+10, 357)):
        try:
            decoded = enc.decode([j])
            # åªæ˜¾ç¤ºå¯æ‰“å°çš„
            if decoded.isprintable() and len(decoded) <= 20:
                tokens.append(f"{j}:{decoded}")
        except:
            pass
    if tokens:
        print(" | ".join(tokens))

# æŸ¥çœ‹æœ€å 20 ä¸ª tokens (åŒ…æ‹¬ç‰¹æ®Š tokens)
print("\nğŸ¯ æœ€å 20 ä¸ª Tokens:")
print("-" * 80)
for i in range(vocab_size - 20, vocab_size):
    try:
        decoded = enc.decode([i])
        # æ£€æŸ¥æ˜¯å¦æ˜¯ç‰¹æ®Š token
        is_special = decoded in special_tokens
        marker = "â­" if is_special else "  "
        print(f"{marker} ID {i:6d}: {repr(decoded)}")
    except:
        print(f"   ID {i:6d}: <decode error>")

# äº¤äº’å¼æŸ¥è¯¢
print("\n" + "=" * 80)
print("\nğŸ’¡ ä½ å¯ä»¥ç”¨è¿™ä¸ªè„šæœ¬æŸ¥çœ‹ç‰¹å®š token:")
print("\nç¤ºä¾‹ä»£ç :")
print("""
import pickle
with open(r"~/.cache/nanochat/tokenizer/tokenizer.pkl", "rb") as f:
    enc = pickle.load(f)

# ç¼–ç æ–‡æœ¬
token_ids = enc.encode("Hello world!")
print(f"Token IDs: {token_ids}")

# æŸ¥çœ‹æ¯ä¸ª token
for token_id in token_ids:
    decoded = enc.decode([token_id])
    print(f"  {token_id:5d} -> {repr(decoded)}")
""")
