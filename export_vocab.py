"""
å¯¼å‡º tokenizer è¯æ±‡è¡¨åˆ°æ–‡æœ¬æ–‡ä»¶ï¼Œæ–¹ä¾¿æŸ¥çœ‹
"""
from nanochat.tokenizer import get_tokenizer
import os

# åŠ è½½ tokenizer
print("Loading tokenizer...")
tokenizer = get_tokenizer()
vocab_size = tokenizer.get_vocab_size()
print(f"Vocab size: {vocab_size:,} tokens")

# å¯¼å‡ºè·¯å¾„
output_file = "vocab_export.txt"
print(f"Exporting vocabulary to: {output_file}")

with open(output_file, "w", encoding="utf-8") as f:
    # å†™å…¥å¤´éƒ¨ä¿¡æ¯
    f.write("=" * 100 + "\n")
    f.write("Nanochat Tokenizer Vocabulary\n")
    f.write(f"Total tokens: {vocab_size:,}\n")
    f.write("=" * 100 + "\n\n")

    # ç‰¹æ®Š tokens
    f.write("ğŸ”– SPECIAL TOKENS\n")
    f.write("-" * 100 + "\n")
    special_tokens = tokenizer.get_special_tokens()
    for special in special_tokens:
        token_id = tokenizer.encode_special(special)
        f.write(f"ID {token_id:6d}: {special}\n")
    f.write("\n")

    # å•å­—èŠ‚ tokens (0-255)
    f.write("ğŸ“ SINGLE BYTE TOKENS (0-255)\n")
    f.write("-" * 100 + "\n")
    for i in range(256):
        try:
            decoded = tokenizer.decode([i])
            # è½¬ä¹‰ç‰¹æ®Šå­—ç¬¦
            if decoded.isprintable() and decoded not in [' ', '\t', '\n', '\r']:
                repr_str = decoded
            else:
                repr_str = repr(decoded)
            f.write(f"ID {i:6d}: {repr_str:20s}  (byte: {i:3d} = 0x{i:02X})\n")
        except:
            f.write(f"ID {i:6d}: <decode error>\n")
    f.write("\n")

    # Merged tokens (256 åˆ° vocab_size-1)
    f.write(f"ğŸ”¤ MERGED TOKENS (256 - {vocab_size-1})\n")
    f.write("-" * 100 + "\n")
    f.write("Showing first 500 and last 100 merged tokens...\n\n")

    # å‰ 500 ä¸ª merged tokens
    for i in range(256, min(756, vocab_size)):
        try:
            decoded = tokenizer.decode([i])
            # é™åˆ¶æ˜¾ç¤ºé•¿åº¦
            if len(decoded) > 50:
                decoded_show = decoded[:47] + "..."
            else:
                decoded_show = decoded
            f.write(f"ID {i:6d}: {repr(decoded_show)}\n")
        except:
            f.write(f"ID {i:6d}: <decode error>\n")

    if vocab_size > 756:
        f.write(f"\n... (skipped {vocab_size - 856} tokens) ...\n\n")

        # æœ€å 100 ä¸ª tokens
        for i in range(max(256, vocab_size - 100), vocab_size):
            try:
                decoded = tokenizer.decode([i])
                if len(decoded) > 50:
                    decoded_show = decoded[:47] + "..."
                else:
                    decoded_show = decoded

                # æ ‡è®°ç‰¹æ®Š token
                is_special = decoded in special_tokens
                marker = "â­" if is_special else "  "
                f.write(f"{marker} ID {i:6d}: {repr(decoded_show)}\n")
            except:
                f.write(f"   ID {i:6d}: <decode error>\n")

print(f"\nâœ… Vocabulary exported to: {os.path.abspath(output_file)}")
print(f"   Total: {vocab_size:,} tokens")
print(f"\nğŸ’¡ Use any text editor to view the file!")
print(f"   Example: notepad {output_file}")
